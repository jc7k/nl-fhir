#!/usr/bin/env python3
"""
Realistic Clinical Text F1 Validation
Tests MedSpaCy performance across complexity levels to validate overfitting hypothesis
"""

import json
import asyncio
import logging
from typing import Dict, List, Any
from datetime import datetime
import time

from src.nl_fhir.services.nlp.pipeline import nlp_pipeline
from src.nl_fhir.services.conversion import ConversionService

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class F1ValidationTester:
    """Tests F1 performance across different clinical text complexity levels"""

    def __init__(self):
        self.conversion_service = ConversionService()
        self.results = {
            "clean": {"correct": 0, "total": 0, "cases": []},
            "realistic": {"correct": 0, "total": 0, "cases": []},
            "complex": {"correct": 0, "total": 0, "cases": []},
            "clinical_trial": {"correct": 0, "total": 0, "cases": []}
        }

    def load_test_cases(self) -> Dict[str, List[Dict]]:
        """Load all generated test cases"""

        test_cases = {}

        # Load synthetic cases
        try:
            with open('realistic_clinical_test_cases_20250914_093841.json', 'r') as f:
                synthetic_data = json.load(f)
                test_cases['clean'] = synthetic_data.get('clean', [])[:10]  # Limit for testing
                test_cases['realistic'] = synthetic_data.get('realistic', [])[:15]
                test_cases['complex'] = synthetic_data.get('complex', [])[:10]
        except FileNotFoundError:
            logger.warning("Synthetic test cases file not found")

        # Load clinical trial cases
        try:
            with open('clinical_trials_test_cases_20250914_094227.json', 'r') as f:
                trial_data = json.load(f)
                test_cases['clinical_trial'] = trial_data[:10]  # Limit for testing
        except FileNotFoundError:
            logger.warning("Clinical trial test cases file not found")

        return test_cases

    async def extract_entities_from_text(self, text: str) -> Dict[str, Any]:
        """Extract entities using our NLP pipeline directly"""
        try:
            # Use the NLP pipeline directly for entity extraction
            result = await nlp_pipeline.process_clinical_text(text, request_id=f"test_{int(time.time())}")

            # Extract entities from the result
            entities = {}

            # Parse from extracted_entities
            if result.get('extracted_entities', {}).get('entities'):
                entity_list = result['extracted_entities']['entities']

                for entity in entity_list:
                    entity_type = entity.get('type', '').lower()
                    entity_text = entity.get('text', '').lower().strip()

                    # Map entity types to expected format
                    if entity_type == 'medication' and 'medication' not in entities:
                        entities['medication'] = entity_text
                    elif entity_type == 'dosage' and 'dosage' not in entities:
                        entities['dosage'] = entity.get('text', '')
                    elif entity_type == 'frequency' and 'frequency' not in entities:
                        entities['frequency'] = entity.get('text', '')
                    elif entity_type == 'condition' and 'condition' not in entities:
                        entities['condition'] = entity_text
                    elif entity_type == 'person' and 'patient' not in entities:
                        entities['patient'] = entity.get('text', '')

            # Also check structured output for additional entities
            if result.get('structured_output', {}).get('structured_output'):
                structured = result['structured_output']['structured_output']

                # Parse structured medication info
                if structured.get('medications'):
                    for med in structured['medications']:
                        if 'medication' not in entities and med.get('name'):
                            entities['medication'] = med['name'].lower()
                        if 'dosage' not in entities and med.get('dosage'):
                            entities['dosage'] = med['dosage']
                        if 'frequency' not in entities and med.get('frequency'):
                            entities['frequency'] = med['frequency']

                # Parse structured patient info
                if structured.get('patient', {}).get('name') and 'patient' not in entities:
                    entities['patient'] = structured['patient']['name']

                # Parse structured conditions
                if structured.get('conditions'):
                    for condition in structured['conditions']:
                        if 'condition' not in entities and condition.get('name'):
                            entities['condition'] = condition['name'].lower()

            return entities

        except Exception as e:
            logger.error(f"Entity extraction failed for text: {text[:50]}... Error: {e}")
            return {}

    def calculate_entity_f1(self, expected: Dict[str, Any], extracted: Dict[str, Any]) -> float:
        """Calculate F1 score for entity extraction"""

        if not expected:
            return 0.0

        # Compare each expected entity
        matches = 0
        total_expected = len(expected)

        for entity_type, expected_value in expected.items():
            if entity_type in extracted:
                extracted_value = extracted[entity_type]

                # Normalize strings for comparison
                if isinstance(expected_value, str) and isinstance(extracted_value, str):
                    expected_norm = expected_value.lower().strip()
                    extracted_norm = extracted_value.lower().strip()

                    # Check for exact match or substring match
                    if (expected_norm == extracted_norm or
                        expected_norm in extracted_norm or
                        extracted_norm in expected_norm):
                        matches += 1
                else:
                    if expected_value == extracted_value:
                        matches += 1

        # F1 score calculation
        if total_expected == 0:
            return 0.0

        precision = matches / max(len(extracted), 1)
        recall = matches / total_expected

        if precision + recall == 0:
            return 0.0

        f1 = 2 * (precision * recall) / (precision + recall)
        return f1

    async def test_complexity_level(self, complexity: str, test_cases: List[Dict]) -> Dict[str, Any]:
        """Test F1 performance for a specific complexity level"""

        logger.info(f"Testing {complexity} complexity level with {len(test_cases)} cases")

        results = {
            "complexity": complexity,
            "total_cases": len(test_cases),
            "f1_scores": [],
            "case_details": [],
            "average_f1": 0.0,
            "processing_errors": 0
        }

        for i, case in enumerate(test_cases):
            try:
                text = case.get('text', '')
                expected = case.get('expected', {})

                logger.info(f"[{complexity}] Testing case {i+1}/{len(test_cases)}")
                logger.info(f"[{complexity}] Text: {text[:100]}...")

                # Extract entities
                extracted = await self.extract_entities_from_text(text)

                # Calculate F1
                f1_score = self.calculate_entity_f1(expected, extracted)

                case_result = {
                    "case_id": case.get('case_id', f'{complexity}_{i+1}'),
                    "text": text[:200] + "..." if len(text) > 200 else text,
                    "expected": expected,
                    "extracted": extracted,
                    "f1_score": f1_score
                }

                results["f1_scores"].append(f1_score)
                results["case_details"].append(case_result)

                logger.info(f"[{complexity}] Case F1: {f1_score:.3f}")

                # Rate limiting to avoid overwhelming the system
                await asyncio.sleep(0.5)

            except Exception as e:
                logger.error(f"[{complexity}] Error processing case {i+1}: {e}")
                results["processing_errors"] += 1

        # Calculate average F1
        if results["f1_scores"]:
            results["average_f1"] = sum(results["f1_scores"]) / len(results["f1_scores"])

        logger.info(f"[{complexity}] Average F1: {results['average_f1']:.3f}")

        return results

    async def run_comprehensive_validation(self) -> Dict[str, Any]:
        """Run comprehensive F1 validation across all complexity levels"""

        logger.info("ğŸ§ª Starting Comprehensive F1 Validation")
        logger.info("Testing MedSpaCy performance across clinical text complexity levels")

        # Initialize services
        if not nlp_pipeline.initialized:
            logger.info("Initializing NLP pipeline...")
            nlp_pipeline.initialize()

        # Load test cases
        test_cases = self.load_test_cases()

        validation_results = {
            "test_timestamp": datetime.now().isoformat(),
            "complexity_results": {},
            "summary": {
                "total_cases_tested": 0,
                "overall_average_f1": 0.0,
                "complexity_ranking": []
            }
        }

        # Test each complexity level
        for complexity, cases in test_cases.items():
            if cases:
                result = await self.test_complexity_level(complexity, cases)
                validation_results["complexity_results"][complexity] = result
                validation_results["summary"]["total_cases_tested"] += len(cases)

        # Calculate overall metrics
        all_f1_scores = []
        complexity_averages = []

        for complexity, result in validation_results["complexity_results"].items():
            all_f1_scores.extend(result["f1_scores"])
            complexity_averages.append({
                "complexity": complexity,
                "average_f1": result["average_f1"],
                "case_count": result["total_cases"]
            })

        if all_f1_scores:
            validation_results["summary"]["overall_average_f1"] = sum(all_f1_scores) / len(all_f1_scores)

        # Rank complexities by performance
        validation_results["summary"]["complexity_ranking"] = sorted(
            complexity_averages,
            key=lambda x: x["average_f1"],
            reverse=True
        )

        return validation_results

    def save_results(self, results: Dict[str, Any]):
        """Save validation results to file"""

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"f1_validation_results_{timestamp}.json"

        with open(filename, 'w') as f:
            json.dump(results, f, indent=2)

        logger.info(f"ğŸ’¾ Results saved to: {filename}")
        return filename

async def main():
    """Run the F1 validation test"""

    tester = F1ValidationTester()

    try:
        results = await tester.run_comprehensive_validation()
        filename = tester.save_results(results)

        # Print summary
        print(f"\nğŸ¯ F1 VALIDATION RESULTS")
        print(f"{'='*50}")
        print(f"Total test cases: {results['summary']['total_cases_tested']}")
        print(f"Overall average F1: {results['summary']['overall_average_f1']:.3f}")
        print(f"\nğŸ“Š Performance by Complexity:")

        for rank in results['summary']['complexity_ranking']:
            complexity = rank['complexity']
            f1 = rank['average_f1']
            count = rank['case_count']
            status = "âœ… GOOD" if f1 > 0.6 else "âš ï¸  NEEDS WORK" if f1 > 0.3 else "âŒ POOR"
            print(f"   {complexity:15} F1: {f1:.3f} ({count} cases) {status}")

        # Key insights
        print(f"\nğŸ’¡ KEY INSIGHTS:")
        best = results['summary']['complexity_ranking'][0]
        worst = results['summary']['complexity_ranking'][-1]

        print(f"   â€¢ Best Performance: {best['complexity']} (F1: {best['average_f1']:.3f})")
        print(f"   â€¢ Worst Performance: {worst['complexity']} (F1: {worst['average_f1']:.3f})")

        if results['summary']['overall_average_f1'] < 0.6:
            print(f"   â€¢ ğŸš¨ VALIDATION CONFIRMS: F1 issues are NOT due to overfitting")
            print(f"   â€¢ Performance drops significantly on realistic clinical text")
            print(f"   â€¢ Configuration optimization approach is justified")
        else:
            print(f"   â€¢ âœ… System performs well across complexity levels")

        return results

    except Exception as e:
        logger.error(f"Validation test failed: {e}")
        return None

if __name__ == "__main__":
    asyncio.run(main())