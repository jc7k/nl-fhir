# Story 4.2: LLM Integration with Structured Output

## Story Overview

**Epic:** Epic 4 - FHIR Bundle Summarization  
**Story ID:** 4.2  
**Title:** LLM Integration with Structured Output  
**Status:** Ready for Development  
**Estimate:** 8 story points  
**Sprint:** Sprint 4 Week 2  

## User Story

**As a** physician reviewing FHIR bundle summaries  
**I want** consistent, structured clinical summaries generated by LLM  
**So that** I receive reliable, professional clinical language in a predictable format every time  

## Acceptance Criteria

### AC1: Instructor + OpenAI Integration
- [ ] **GIVEN** a selected Pydantic schema and FHIR bundle
- [ ] **WHEN** the system calls the LLM with structured output constraints
- [ ] **THEN** it returns a valid Pydantic model instance with populated fields
- [ ] **AND** the response validates against the schema without errors

### AC2: Role-Based Prompt Customization  
- [ ] **GIVEN** different clinical roles (physician, nurse, pharmacist)
- [ ] **WHEN** generating clinical summaries
- [ ] **THEN** the prompt adapts language and emphasis for the target audience
- [ ] **AND** maintains consistent structured output regardless of role

### AC3: Natural Clinical Language Generation
- [ ] **GIVEN** structured FHIR data in the prompt
- [ ] **WHEN** the LLM generates summary content
- [ ] **THEN** all field values use natural, professional clinical language
- [ ] **AND** avoid technical jargon or machine-readable formats

### AC4: Error Handling and Fallback
- [ ] **GIVEN** LLM API failures or schema validation errors
- [ ] **WHEN** structured output generation fails
- [ ] **THEN** the system provides graceful error handling
- [ ] **AND** returns appropriate error messages with fallback options

## Technical Requirements

### LLM Service Implementation

**Core Service Class:**
```python
class StructuredClinicalSummarizer:
    def __init__(self):
        self.client = instructor.patch(AsyncOpenAI())
        self.schema_selector = SchemaSelector()
        
    async def generate_structured_summary(
        self, 
        fhir_bundle: Dict[str, Any],
        clinical_role: str = "physician",
        request_id: Optional[str] = None
    ) -> Dict[str, Any]:
        # Main summarization workflow
```

**Instructor Integration:**
```python
response = await self.client.chat.completions.create(
    model="gpt-4",
    response_model=selected_schema,  # Pydantic model constrains output
    messages=[{"role": "user", "content": prompt}],
    temperature=0.1  # Low temperature for consistency
)
```

### Role-Based Prompt Generation

**Physician Prompts:**
- Focus on clinical decision-making and medical accuracy
- Include safety considerations and drug interactions
- Emphasize diagnostic reasoning and treatment rationale

**Nurse Prompts:**  
- Emphasize patient care instructions and administration details
- Focus on monitoring requirements and safety protocols
- Include patient education points

**Pharmacist Prompts:**
- Highlight medication safety and drug interactions
- Focus on dosing accuracy and pharmaceutical considerations
- Include dispensing and counseling points

### Prompt Template Structure

```python
def create_role_based_prompt(
    clinical_elements: Dict[str, Any], 
    clinical_role: str,
    schema_class: type[BaseModel]
) -> str:
    
    base_prompt = f"""
You are creating a clinical summary for a {clinical_role} to review and verify 
that orders were correctly interpreted from natural language.

CRITICAL REQUIREMENTS:
- Use clear, natural clinical language appropriate for a {clinical_role}
- Be precise about dosages, frequencies, and clinical indications
- Include safety considerations relevant to {clinical_role} practice
- Make it easy to quickly verify accuracy and appropriateness

CLINICAL ELEMENTS TO SUMMARIZE:
{json.dumps(clinical_elements, indent=2)}

Create a summary that follows the required structure but reads naturally 
for {clinical_role} clinical review and verification.
"""
    
    # Add role-specific guidance
    role_guidance = get_role_specific_guidance(clinical_role, schema_class)
    return f"{base_prompt}\n\n{role_guidance}".strip()
```

## Implementation Details

### File Structure
```
src/nl_fhir/services/
├── structured_clinical_summarizer.py  # Main LLM service
├── llm_prompt_builder.py              # Role-based prompt generation
├── instructor_client.py                # OpenAI + Instructor wrapper
└── llm_error_handler.py               # Error handling and fallbacks
```

### Key Configuration

**OpenAI Settings:**
```python
LLM_CONFIG = {
    "model": "gpt-4",  # Primary model for medical accuracy
    "temperature": 0.1,  # Low temperature for consistency
    "max_tokens": 2000,  # Sufficient for clinical summaries
    "timeout": 30,  # 30-second timeout for API calls
}
```

**Instructor Configuration:**
```python
client = instructor.patch(
    AsyncOpenAI(api_key=settings.OPENAI_API_KEY),
    mode=instructor.Mode.JSON  # Ensure JSON response format
)
```

### Error Handling Strategy

**LLM API Failures:**
- Timeout handling with retry logic (max 3 attempts)
- Rate limit detection and exponential backoff
- Clear error messages for different failure types

**Schema Validation Errors:**
- Pydantic validation error logging and analysis
- Graceful degradation to simplified summary structure
- Developer notifications for schema compatibility issues

**Fallback Mechanisms:**
```python
async def generate_with_fallback(self, fhir_bundle: Dict, **kwargs) -> Dict:
    try:
        return await self.generate_structured_summary(fhir_bundle, **kwargs)
    except OpenAIError as e:
        return self.create_api_error_response(e)
    except ValidationError as e:
        return self.create_validation_error_response(e)
    except Exception as e:
        return self.create_generic_error_response(e)
```

## Testing Requirements

### Unit Tests
- [ ] Test Instructor + OpenAI integration with mock responses
- [ ] Validate prompt generation for different roles and schemas
- [ ] Test error handling for various failure scenarios
- [ ] Verify schema validation with valid/invalid LLM responses

### Integration Tests
- [ ] End-to-end LLM calls with real API (rate-limited test suite)
- [ ] Test role-based customization produces appropriate language
- [ ] Validate consistency across multiple calls with same input
- [ ] Test error handling with simulated API failures

### Performance Tests
- [ ] Measure response times for different bundle sizes
- [ ] Test concurrent request handling
- [ ] Validate memory usage under load
- [ ] Monitor API rate limit compliance

### Test Data Requirements
- Sample FHIR bundles for each schema type
- Mock LLM responses for offline testing
- Error response samples for failure scenario testing
- Role-specific validation criteria

## Definition of Done

- [ ] Instructor + OpenAI integration working with structured output constraints
- [ ] Role-based prompt generation produces appropriate clinical language
- [ ] Error handling covers all identified failure scenarios
- [ ] LLM responses consistently validate against Pydantic schemas
- [ ] Performance meets <500ms target for typical bundles
- [ ] Unit tests achieve >90% code coverage
- [ ] Integration tests validate LLM API functionality
- [ ] Error logging and monitoring implemented
- [ ] Code review completed and approved
- [ ] API key management and security review completed

## Dependencies

**Blocked By:**
- Story 4.1 (Structured Clinical Summary Framework) - requires Pydantic models

**Blocks:**
- Story 4.4 (Clinical Summary API) - requires LLM service implementation

**External Dependencies:**
- OpenAI API access and rate limits
- Instructor library compatibility with OpenAI client
- Environment variables for API key management

## Configuration Requirements

### Environment Variables
```bash
OPENAI_API_KEY=sk-...                    # OpenAI API access
LLM_MODEL=gpt-4                         # Model selection
LLM_TEMPERATURE=0.1                     # Consistency setting
LLM_MAX_TOKENS=2000                     # Response size limit
LLM_TIMEOUT=30                          # Request timeout
LLM_MAX_RETRIES=3                       # Retry attempts
```

### Rate Limiting
- Implement request queuing for high-volume usage
- Monitor API usage to stay within rate limits
- Consider OpenAI tier limits for production deployment

## Notes for Development

**Key Implementation Points:**
1. **Instructor First:** Always use Instructor to constrain LLM output
2. **Low Temperature:** Use 0.1 temperature for consistency
3. **Comprehensive Prompts:** Include all necessary clinical context
4. **Robust Error Handling:** Graceful degradation for all failure modes

**Testing Strategy:**
1. Mock-based unit tests for rapid development iteration
2. Limited integration tests with real API for validation
3. Performance testing to ensure <500ms response times

**Security Considerations:**
- Never log API keys or sensitive clinical data
- Use environment variables for all configuration
- Implement request/response logging for debugging (scrubbed data only)

**Clinical Review Requirements:**
- Physician validation of role-based prompt differences
- Review of generated summaries for clinical accuracy
- Confirmation that structured output supports clinical workflows