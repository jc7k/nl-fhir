# Story 4.4: Generic Template Engine and LLM Integration

## Story Overview

**Epic:** Epic 4 - FHIR Bundle Summarization with Adaptive Cost Optimization  
**Story ID:** 4.4  
**Title:** Generic Template Engine and LLM Integration  
**Status:** Ready for Development  
**Estimate:** 8 story points  
**Sprint:** Sprint 4 Week 4  

## User Story

**As a** healthcare system architect  
**I want** intelligent template-based processing and structured LLM integration as cost-effective fallbacks  
**So that** we can handle complex and novel FHIR resource types while maintaining cost control and consistent output quality  

## Acceptance Criteria

### AC1: Generic Template Engine (Tier 2 Processing)
- [ ] **GIVEN** FHIR resources not covered by rule-based summarizers  
- [ ] **WHEN** the bundle contains moderately complex or mixed resource types
- [ ] **THEN** system uses generic templates to generate clinical summaries without LLM costs
- [ ] **AND** maintains consistent output format and clinical appropriateness for 15-20% of requests

### AC2: Structured LLM Integration (Tier 3 Fallback)  
- [ ] **GIVEN** highly complex bundles or novel resource types that exceed template capabilities
- [ ] **WHEN** rule-based and template processing cannot handle the resource complexity
- [ ] **THEN** system uses Instructor-constrained LLM processing for structured, reliable output
- [ ] **AND** limits LLM usage to target <10% of total processing requests

### AC3: Performance Optimization and Caching
- [ ] **GIVEN** repeated processing of similar FHIR bundle patterns
- [ ] **WHEN** template or LLM processing generates summaries
- [ ] **THEN** system implements intelligent caching to reduce processing time and costs
- [ ] **AND** maintains <500ms response time across template and LLM processing tiers

### AC4: Quality Assurance and Graceful Degradation
- [ ] **GIVEN** LLM service outages or processing failures
- [ ] **WHEN** structured LLM processing is unavailable
- [ ] **THEN** system gracefully degrades to generic template processing with clear quality indicators
- [ ] **AND** maintains service availability while logging degradation events for monitoring

## Technical Requirements

### Generic Template Engine (Tier 2)

**GenericTemplateEngine:**
```python
class GenericTemplateEngine:
    """
    Template-based FHIR resource processing for moderately complex bundles
    Bridges gap between rule-based processing and LLM fallback
    """
    
    def __init__(self):
        self.resource_classifiers = ResourceTypeClassifier()
        self.template_selector = DynamicTemplateSelector() 
        self.clinical_formatter = ClinicalLanguageFormatter()
        self.cache_manager = TemplateCacheManager()
    
    async def process_bundle(self, fhir_bundle: Dict[str, Any], 
                           role: str = "physician") -> ClinicalSummary:
        """Process bundle using generic template-based approach"""
        
        # Classify resources and select appropriate templates
        resource_classification = await self._classify_bundle_resources(fhir_bundle)
        templates = await self.template_selector.select_templates(resource_classification)
        
        # Generate summaries using templates
        resource_summaries = []
        for resource in fhir_bundle.get('entry', []):
            summary = await self._process_resource_with_template(resource, templates)
            resource_summaries.append(summary)
        
        # Compose final clinical summary
        return await self._compose_clinical_summary(resource_summaries, role)
    
    async def _process_resource_with_template(self, resource: Dict, 
                                            templates: TemplateSet) -> ResourceSummary:
        """Apply appropriate template to individual FHIR resource"""
```

**DynamicTemplateSelector:**
```python
class DynamicTemplateSelector:
    """
    Intelligent template selection based on resource characteristics
    Provides appropriate clinical language patterns for unknown resource types
    """
    
    GENERIC_RESOURCE_TEMPLATES = {
        'observation': {
            'vital_signs': "Patient vital signs: {display_name} = {value} {unit}",
            'lab_result': "Laboratory result: {test_name} = {value} {unit} ({interpretation})",
            'clinical_finding': "Clinical observation: {finding} - {status}"
        },
        'diagnostic_report': {
            'imaging': "Imaging study: {study_type} - {findings}",
            'pathology': "Pathology report: {specimen_type} - {diagnosis}",  
            'generic': "Diagnostic report: {title} - {conclusion}"
        },
        'care_plan': {
            'treatment': "Treatment plan: {description} - {goals}",
            'monitoring': "Care monitoring: {activities} - {target_outcomes}"
        },
        'unknown_resource': {
            'fallback': "Clinical order: {resource_type} - {description}"
        }
    }
    
    async def select_templates(self, classification: ResourceClassification) -> TemplateSet:
        """Select appropriate templates based on resource analysis"""
        
    async def _generate_adaptive_template(self, resource_type: str, 
                                        resource_data: Dict) -> str:
        """Generate template for previously unseen resource patterns"""
```

### Structured LLM Integration (Tier 3)

**StructuredLLMService:**
```python
class StructuredLLMService:
    """
    Instructor-constrained LLM integration for complex clinical summarization
    Provides high-quality fallback with structured, validated output
    """
    
    def __init__(self):
        self.instructor_client = self._initialize_instructor_client()
        self.cost_tracker = LLMCostTracker()
        self.quality_validator = LLMOutputValidator()
        self.prompt_optimizer = ClinicalPromptOptimizer()
    
    async def process_complex_bundle(self, fhir_bundle: Dict[str, Any],
                                   role: str = "physician") -> ClinicalSummary:
        """Process complex bundle using structured LLM with Instructor constraints"""
        
        # Track cost impact
        await self.cost_tracker.start_llm_request(fhir_bundle)
        
        try:
            # Generate structured prompt with clinical context
            structured_prompt = await self.prompt_optimizer.generate_clinical_prompt(
                fhir_bundle, role
            )
            
            # Use Instructor for structured, validated output
            summary = await self.instructor_client.create(
                model="gpt-4o-mini",
                response_model=ClinicalSummary,
                messages=[
                    {"role": "system", "content": self._get_clinical_system_prompt()},
                    {"role": "user", "content": structured_prompt}
                ],
                temperature=0.1,
                max_tokens=2000
            )
            
            # Validate output quality
            await self.quality_validator.validate_clinical_summary(summary)
            
            return summary
            
        finally:
            await self.cost_tracker.complete_llm_request()
    
    def _get_clinical_system_prompt(self) -> str:
        """Clinical system prompt optimized for FHIR bundle summarization"""
        return """You are a clinical assistant generating structured summaries of FHIR bundles.
        Focus on clinical accuracy, safety considerations, and physician-appropriate language.
        Maintain consistency with provided Pydantic model structure.
        Emphasize critical information like dosages, contraindications, and urgent care needs."""
```

**ClinicalSummary Pydantic Models:**
```python
class ClinicalSummary(BaseModel):
    """
    Structured clinical summary model for Instructor-constrained LLM output
    Ensures consistent format across all processing tiers
    """
    
    summary_type: Literal["medication_only", "comprehensive", "emergency", "complex"]
    processing_tier: ProcessingTier
    confidence_score: float = Field(..., ge=0.0, le=1.0)
    
    # Core clinical content
    patient_context: str = Field(..., description="Brief patient context and demographics")
    primary_orders: List[ClinicalOrder] = Field(..., description="Main clinical orders from bundle")
    supporting_information: Optional[str] = Field(None, description="Additional clinical context")
    
    # Safety and quality indicators
    clinical_alerts: List[str] = Field(default_factory=list, description="Safety alerts or contraindications")
    quality_indicators: QualityIndicators = Field(..., description="Summary quality and completeness metrics")
    
    # Role-specific customization
    physician_notes: Optional[str] = Field(None, description="Physician-specific clinical notes")
    nursing_considerations: Optional[str] = Field(None, description="Nursing care considerations") 
    pharmacy_notes: Optional[str] = Field(None, description="Pharmacy review and drug interactions")

class ClinicalOrder(BaseModel):
    """Individual clinical order within the summary"""
    order_type: str = Field(..., description="Type of clinical order")
    description: str = Field(..., description="Natural language order description")
    priority: Optional[str] = Field(None, description="Order priority level")
    clinical_rationale: Optional[str] = Field(None, description="Clinical reasoning for order")

class QualityIndicators(BaseModel):
    """Quality and completeness indicators for the generated summary"""
    completeness_score: float = Field(..., ge=0.0, le=1.0)
    clinical_accuracy_confidence: float = Field(..., ge=0.0, le=1.0) 
    terminology_consistency: float = Field(..., ge=0.0, le=1.0)
    missing_critical_information: bool = Field(...)
```

### Performance Optimization and Caching

**IntelligentCacheManager:**
```python
class IntelligentCacheManager:
    """
    Advanced caching system for template and LLM processing optimization
    Reduces processing time and costs through pattern recognition
    """
    
    def __init__(self):
        self.bundle_hasher = FHIRBundleHasher()
        self.cache_store = RedisCacheStore()  # or in-memory for development
        self.cache_analytics = CacheAnalyticsTracker()
    
    async def get_cached_summary(self, fhir_bundle: Dict[str, Any], 
                               role: str) -> Optional[ClinicalSummary]:
        """Retrieve cached summary for identical or similar bundles"""
        
        bundle_hash = await self.bundle_hasher.generate_semantic_hash(fhir_bundle)
        cache_key = f"summary:{bundle_hash}:{role}"
        
        cached_result = await self.cache_store.get(cache_key)
        if cached_result:
            await self.cache_analytics.record_cache_hit(bundle_hash)
            return ClinicalSummary.model_validate(cached_result)
        
        return None
    
    async def cache_summary(self, fhir_bundle: Dict[str, Any], 
                          role: str, summary: ClinicalSummary):
        """Cache generated summary with intelligent expiration"""
        
        bundle_hash = await self.bundle_hasher.generate_semantic_hash(fhir_bundle)
        cache_key = f"summary:{bundle_hash}:{role}"
        
        # Determine cache TTL based on content type and processing tier
        ttl_seconds = await self._calculate_cache_ttl(summary)
        
        await self.cache_store.set(cache_key, summary.model_dump(), expire=ttl_seconds)
        await self.cache_analytics.record_cache_store(bundle_hash)
```

### Graceful Degradation and Error Handling

**GracefulDegradationController:**
```python
class GracefulDegradationController:
    """
    Handles service failures and implements intelligent fallback strategies
    Maintains service availability during LLM outages or processing errors
    """
    
    def __init__(self):
        self.service_health_monitor = ServiceHealthMonitor()
        self.fallback_coordinator = FallbackCoordinator()
        self.degradation_notifier = DegradationNotifier()
    
    async def handle_llm_service_failure(self, fhir_bundle: Dict[str, Any], 
                                       original_error: Exception) -> ClinicalSummary:
        """Graceful fallback when LLM processing fails"""
        
        await self.degradation_notifier.notify_service_degradation(
            service="llm_processing", 
            error=original_error,
            bundle_id=fhir_bundle.get('id')
        )
        
        # Attempt fallback to generic template processing
        try:
            template_engine = GenericTemplateEngine()
            summary = await template_engine.process_bundle(fhir_bundle)
            
            # Mark summary as degraded quality
            summary.processing_tier = ProcessingTier.GENERIC_TEMPLATE
            summary.quality_indicators.clinical_accuracy_confidence *= 0.8  # Reduce confidence
            summary.clinical_alerts.append("Generated using fallback processing due to service issues")
            
            return summary
            
        except Exception as fallback_error:
            # Final fallback to basic resource listing
            return await self._generate_basic_resource_summary(fhir_bundle)
    
    async def _generate_basic_resource_summary(self, fhir_bundle: Dict[str, Any]) -> ClinicalSummary:
        """Minimal fallback summary with basic resource listing"""
        
        resources = [entry.get('resource', {}) for entry in fhir_bundle.get('entry', [])]
        resource_types = [r.get('resourceType', 'Unknown') for r in resources]
        
        return ClinicalSummary(
            summary_type="complex",
            processing_tier=ProcessingTier.EMERGENCY_FALLBACK,
            confidence_score=0.3,
            patient_context="Bundle processing encountered technical issues",
            primary_orders=[
                ClinicalOrder(
                    order_type=rt,
                    description=f"Clinical order of type {rt} - detailed processing unavailable"
                ) for rt in set(resource_types)
            ],
            clinical_alerts=["Summary generated using emergency fallback - manual review recommended"],
            quality_indicators=QualityIndicators(
                completeness_score=0.3,
                clinical_accuracy_confidence=0.3,
                terminology_consistency=0.5,
                missing_critical_information=True
            )
        )
```

## Implementation Details

### File Structure
```
src/nl_fhir/services/summarization/
├── generic_template_engine.py        # Tier 2: Generic template processing
├── structured_llm_service.py         # Tier 3: LLM integration with Instructor
├── templates/                        # Template management
│   ├── dynamic_template_selector.py  # Template selection logic
│   ├── generic_templates.py          # Generic clinical templates
│   └── adaptive_template_generator.py # Runtime template generation
├── llm_integration/                  # LLM service components
│   ├── instructor_client.py          # Instructor setup and configuration
│   ├── clinical_prompt_optimizer.py  # Prompt engineering for clinical context
│   ├── llm_cost_tracker.py          # Cost tracking and budget management
│   └── output_validator.py           # LLM output quality validation
├── caching/                          # Performance optimization
│   ├── intelligent_cache_manager.py  # Advanced caching system
│   ├── bundle_hasher.py              # Semantic FHIR bundle hashing
│   └── cache_analytics.py            # Cache performance tracking
├── graceful_degradation/             # Error handling and fallbacks
│   ├── degradation_controller.py     # Service degradation management
│   ├── fallback_coordinator.py       # Fallback strategy coordination
│   └── service_health_monitor.py     # Service availability monitoring
└── models/                           # Shared models and schemas
    ├── clinical_summary_models.py    # Pydantic models for structured output
    ├── template_models.py             # Template selection and management models
    └── quality_models.py              # Quality assessment and validation models
```

## Testing Requirements

### Unit Tests
- [ ] Test generic template selection and application with various resource types
- [ ] Validate Instructor-constrained LLM output structure and clinical content
- [ ] Test intelligent caching with bundle similarity detection
- [ ] Verify graceful degradation during service failures

### Integration Tests  
- [ ] End-to-end processing through template and LLM tiers
- [ ] Cost tracking validation and budget enforcement testing
- [ ] Cache effectiveness testing with real bundle variations
- [ ] Service degradation scenarios with fallback validation

### Performance Tests
- [ ] Template processing performance with complex bundles (<500ms target)
- [ ] LLM processing optimization and cost per request measurement
- [ ] Cache hit rate optimization and memory usage efficiency
- [ ] High availability testing during service outages

### Clinical Validation Tests
- [ ] Physician review of template-generated summaries for clinical appropriateness
- [ ] LLM output quality assessment compared to rule-based processing
- [ ] Fallback summary usefulness during service degradation scenarios

## Definition of Done

- [ ] Generic template engine processes 15-20% of bundles without LLM costs
- [ ] Structured LLM service provides high-quality fallback for complex bundles (<10% usage)
- [ ] Intelligent caching reduces processing time and costs for repeated patterns
- [ ] Graceful degradation maintains service availability during LLM outages
- [ ] All Pydantic models ensure structured, consistent output across tiers
- [ ] Cost tracking provides real-time LLM usage and budget monitoring
- [ ] Integration tests validate tier fallback logic and performance requirements
- [ ] Clinical validation confirms summary quality meets physician review standards
- [ ] Code review completed and approved
- [ ] Documentation includes template customization and LLM configuration guides

## Dependencies

**Blocked By:**
- Story 4.1 (Adaptive Framework) - requires tier integration architecture
- Story 4.3 (Monitoring System) - requires cost tracking integration

**Blocks:**
- None (final Epic 4 story)

**External Dependencies:**
- OpenAI API for LLM processing (gpt-4o-mini)
- Instructor library for structured LLM output
- Redis or equivalent for intelligent caching
- Pydantic v2 for output validation

## Notes for Development

**Cost Optimization Strategy:**
- Template engine targets 15-20% of bundles to minimize LLM costs
- LLM processing limited to <10% of requests through intelligent routing
- Caching system reduces repeated processing costs for similar bundles
- Cost tracking enables real-time budget monitoring and alerts

**Quality Assurance:**
- Instructor constraints ensure consistent LLM output structure
- Template system provides clinically appropriate language for known patterns
- Graceful degradation maintains minimum service quality during outages
- Quality indicators enable monitoring of summary completeness and accuracy

**Clinical Safety:**
- Structured output models enforce inclusion of critical clinical information
- Role-based customization ensures appropriate clinical perspectives
- Safety alert integration highlights contraindications and drug interactions
- Fallback processing maintains patient safety even during system degradation

**Scalability and Performance:**
- Async processing architecture supports high-volume production loads
- Intelligent caching reduces processing overhead for repeated bundle patterns  
- Service health monitoring enables proactive degradation management
- Cost protection prevents budget overruns during usage spikes